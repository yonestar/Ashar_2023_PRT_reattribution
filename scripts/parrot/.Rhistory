```{r}
library(devtools)
library(stm)
library(parrot)
processed <- textProcessor(
attr$attributions, attr,
removestopwords=T, lowercase=T, stem=F
)
View(processed)
View(processed)
processed$meta$attributions
?textProcessor
processed$vocab
processed$documents
processed <- textProcessor(
attr$attributions, attr,
removestopwords=T, lowercase=T, stem=F, removepunctuation = TRUE
)
processed$documents
processed$documents[135]
?prepDocuments
out <- prepDocuments(
processed$documents, processed$vocab, processed$meta
)
out$words.removed
processed <- textProcessor(
attr$attributions, attr,
removestopwords=T, lowercase=T, stem=T, removepunctuation = TRUE
)
out <- prepDocuments(
processed$documents, processed$vocab, processed$meta
)
out$words.removed
out$docs.removed
out$vocab
out$meta$attributions[135]
out$vocab[135]
out$documents$135
out$documents[135]
?doc_to_tdm
tdm <- doc_to_tdm(out)
View(tdm)
View(tdm)
embeddings <- read_word_embeddings(
in_vocab=out$vocab,
#ovefile = "/Users/yoni/Repositories/parrot/data/glove.6B/glove.6B.50d.txt"
ovefile = "/Users/yoni/Repositories/parrot/data/O2M_overlap.txt"
# embedding makes no difference in test data, as W Hobbs expects
#"O2M_overlap.txt" # must add location on your computer "path/to/O2M_overlap.txt"
## ovefile2 = "path/to/O2M_oov.txt", # very rare words and misspellings
## available here http://www.cis.uni-muenchen.de/~wenpeng/renamed-meta-emb.tar.gz
## must unpack and replace "path/to/" with location on your computer
)
processed <- textProcessor(
attr$attributions, attr,
removestopwords=T, lowercase=T, stem=F, removepunctuation = TRUE
)
# removes stems that occur only once in the corpus
out <- prepDocuments(
processed$documents, processed$vocab, processed$meta
)
tdm <- doc_to_tdm(out)
embeddings <- read_word_embeddings(
in_vocab=out$vocab,
#ovefile = "/Users/yoni/Repositories/parrot/data/glove.6B/glove.6B.50d.txt"
ovefile = "/Users/yoni/Repositories/parrot/data/O2M_overlap.txt"
# embedding makes no difference in test data, as W Hobbs expects
#"O2M_overlap.txt" # must add location on your computer "path/to/O2M_overlap.txt"
## ovefile2 = "path/to/O2M_oov.txt", # very rare words and misspellings
## available here http://www.cis.uni-muenchen.de/~wenpeng/renamed-meta-emb.tar.gz
## must unpack and replace "path/to/" with location on your computer
)
?read_word_embeddings
?textProcessor
library(devtools)
library(stm)
library(parrot)
# If i stem, then many words are no longer found in the embedding. might be a problem?
# If I don't stem, more words are dropped as ocurring only once (eg factor and factors don't count in same category)
processed <- textProcessor(
attr$attributions, attr,
removestopwords=T, lowercase=T, stem=F, removepunctuation = TRUE
)
# removes words that occur only once in the corpus
out <- prepDocuments(
processed$documents, processed$vocab, processed$meta
)
# at this point, each subject (at T2) is a list of "hits" re: which terms are present across their 3 attributions, and the number of times each term is present for them
tdm <- doc_to_tdm(out)
embeddings <- read_word_embeddings(
in_vocab=out$vocab,
#ovefile = "/Users/yoni/Repositories/parrot/data/glove.6B/glove.6B.50d.txt"
ovefile = "/Users/yoni/Repositories/parrot/data/O2M_overlap.txt"
# embedding makes no difference in test data, as W Hobbs expects
#"O2M_overlap.txt" # must add location on your computer "path/to/O2M_overlap.txt"
## ovefile2 = "path/to/O2M_oov.txt", # very rare words and misspellings
## available here http://www.cis.uni-muenchen.de/~wenpeng/renamed-meta-emb.tar.gz
## must unpack and replace "path/to/" with location on your computer
)
scores <- scale_text(
meta=out$meta,
tdm=tdm
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
document_scores <- score_documents(
scores=scores, n_dimensions=3
)
get_keywords(scores, n_dimensions=3, n_words=15)
with(document_scores, cor(sqrt(n_words), X0, use="complete"))
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.9
)
# If i stem, then many words are no longer found in the embedding. might be a problem?
# If I don't stem, more words are dropped as ocurring only once (eg factor and factors don't count in same category)
processed <- textProcessor(
attr$attributions, attr,
removestopwords=T, lowercase=T, stem=F, removepunctuation = TRUE
)
# removes words that occur only once in the corpus
out <- prepDocuments(
processed$documents, processed$vocab, processed$meta
)
tdm <- doc_to_tdm(out)
embeddings <- read_word_embeddings(
in_vocab=out$vocab,
#ovefile = "/Users/yoni/Repositories/parrot/data/glove.6B/glove.6B.50d.txt"
ovefile = "/Users/yoni/Repositories/parrot/data/O2M_overlap.txt"
# embedding makes no difference in test data, as W Hobbs expects
#"O2M_overlap.txt" # must add location on your computer "path/to/O2M_overlap.txt"
## ovefile2 = "path/to/O2M_oov.txt", # very rare words and misspellings
## available here http://www.cis.uni-muenchen.de/~wenpeng/renamed-meta-emb.tar.gz
## must unpack and replace "path/to/" with location on your computer
)
View(embeddings)
?read_word_embeddings
scores <- scale_text(
meta=out$meta,
tdm=tdm,
embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
scores <- scale_text(
meta=out$meta,
tdm=tdm
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
?scale_text
?score_documents
View(scores)
View(scores)
View(scores)
View(scores)
?scale_text
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 4
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
View(scores)
View(scores)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 6
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 6,
verbose = T
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 6,
verbose = T,
constrain_outliers = T
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
View(scores)
document_scores <- score_documents(
scores=scores, n_dimensions=3
)
get_keywords(scores, n_dimensions=3, n_words=15)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 2,
verbose = T,
constrain_outliers = T
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
document_scores <- score_documents(
scores=scores, n_dimensions=3
)
get_keywords(scores, n_dimensions=3, n_words=15)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 4,
verbose = T,
constrain_outliers = T
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
document_scores <- score_documents(
scores=scores, n_dimensions=3
)
get_keywords(scores, n_dimensions=3, n_words=15)
?score_documents
View(document_scores)
View(document_scores)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 4,
verbose = T,
constrain_outliers = T,
n_dimension_compression = 3
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
View(scores)
document_scores <- score_documents(
scores=scores, n_dimensions=3
)
get_keywords(scores, n_dimensions=3, n_words=15)
get_keywords(scores, n_dimensions=3, n_words=5)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 4,
verbose = T,
constrain_outliers = T,
n_dimension_compression = 5
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
document_scores <- score_documents(
scores=scores, n_dimensions=3
)
get_keywords(scores, n_dimensions=3, n_words=15)
View(scores)
View(scores)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 4,
verbose = T,
constrain_outliers = T,
#    n_dimension_compression = 5
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
document_scores <- score_documents(
scores=scores, n_dimensions=3
)
get_keywords(scores, n_dimensions=3, n_words=15)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 4,
verbose = T,
constrain_outliers = T,
#    n_dimension_compression = 5 # does not look as good
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
document_scores <- score_documents(
scores=scores, n_dimensions=3
)
View(document_scores)
get_keywords(scores, n_dimensions=3, n_words=15)
scores$tdm
scores$word_scores
get_keywords(scores, n_dimensions=3, n_words=8)
get_keywords(scores, n_dimensions=2, n_words=8)
?with
with(document_scores, cor(sqrt(n_words), X0, use="complete"))
?cor
with(document_scores, cor(sqrt(n_words), X1, use="complete"))
# confirming that score on first dimension (X0) is uncorrelated with doc length
with(document_scores, cor(sqrt(n_words), X0, use="complete"))
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.9
)
?plot_keywords
plot_keywords(
scores, x_dimension=0, y_dimension=2, q_cutoff=0.9
)
plot_keywords(
scores, x_dimension=0, y_dimension=20, q_cutoff=0.9
)
plot_keywords(
scores, x_dimension=0, y_dimension=1, q_cutoff=0.9
)
plot_keywords(
scores, x_dimension=0, y_dimension=1, q_cutoff=0.5
)
plot_keywords(
scores, x_dimension=0, y_dimension=1, q_cutoff=0.7
)
plot_keywords(
scores, x_dimension=0, y_dimension=1, q_cutoff=0.77
)
plot_keywords(
scores, x_dimension=0, y_dimension=1, q_cutoff=0.8
)
plot_keywords(
scores, x_dimension=0, y_dimension=1, q_cutoff=0.9
)
plot_keywords(
scores, x_dimension=0, y_dimension=1, q_cutoff=0.8
)
get_keywords(scores, n_dimensions=3, n_words=8)
plot_keywords(
scores, x_dimension=0, y_dimension=1, q_cutoff=0.8
)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8
)
get_keywords(scores, n_dimensions=3, n_words=8)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8
)
color
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.5
)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.6
)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.7
)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8
)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=T,
)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, unstretch=T
)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, unstretch=1
)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, unstretch=TRUE
)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
library(parrot)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
library(parrot)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
library(devtools)
library(devtools)
library(stm)
# If i stem, then many words are no longer found in the embedding. might be a problem?
# If I don't stem, more words are dropped as ocurring only once (eg factor and factors don't count in same category)
processed <- textProcessor(
attr$attributions, attr,
removestopwords=T, lowercase=T, stem=F, removepunctuation = TRUE
)
# removes words that occur only once in the corpus
out <- prepDocuments(
processed$documents, processed$vocab, processed$meta
)
tdm <- doc_to_tdm(out)
library(readr)
library(devtools)
library(stm)
library(parrot)
# If i stem, then many words are no longer found in the embedding. might be a problem?
# If I don't stem, more words are dropped as ocurring only once (eg factor and factors don't count in same category)
processed <- textProcessor(
attr$attributions, attr,
removestopwords=T, lowercase=T, stem=F, removepunctuation = TRUE
)
# removes words that occur only once in the corpus
out <- prepDocuments(
processed$documents, processed$vocab, processed$meta
)
tdm <- doc_to_tdm(out)
# doesn't find 2 words. not sure which. prob doesn't matter. esp since embeddings are not even used below!
embeddings <- read_word_embeddings(
in_vocab=out$vocab,
#ovefile = "/Users/yoni/Repositories/parrot/data/glove.6B/glove.6B.50d.txt"
ovefile = "/Users/yoni/Repositories/parrot/data/O2M_overlap.txt"
# embedding makes no difference in test data, as W Hobbs expects
#"O2M_overlap.txt" # must add location on your computer "path/to/O2M_overlap.txt"
## ovefile2 = "path/to/O2M_oov.txt", # very rare words and misspellings
## available here http://www.cis.uni-muenchen.de/~wenpeng/renamed-meta-emb.tar.gz
## must unpack and replace "path/to/" with location on your computer
)
# generates a score for each word.
# work better if lemmatized? if using embeddings?
scores <- scale_text(
meta=out$meta,
tdm=tdm,
pivot = 4,
verbose = T,
constrain_outliers = T,
#    n_dimension_compression = 5 # does not look as good
##    embeddings=embeddings[["meta"]], ## embeddings have little effect
##    on output -- if used, consider setting pivot lower (e.g. pivot = 1/2)
)
document_scores <- score_documents(
scores=scores, n_dimensions=3
)
get_keywords(scores, n_dimensions=3, n_words=8)
# confirming that score on first dimension (X0) is uncorrelated with doc length
with(document_scores, cor(sqrt(n_words), X0, use="complete"))
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
library(parrot)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
reload(pkg='parrot')
reload(parrot)
unload(parrot)
unload('parrot)
unload('parrot')
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
?unload
library(parrot)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
reload(parrot)
reload('parrot')
detach("package:parrot", unload=TRUE)
library(parrot)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
detach("package:parrot", unload=TRUE)
library(parrot)
library(parrot, verbose=T)
detach("package:parrot", unload=TRUE)
library(parrot, verbose=T)
# install locally (with edits)
install.packages("/Users/yoni/Repositories/parrot", repos=NULL, type="source")
# install locally (with edits)
install.packages("/Users/yoni/Repositories/parrot", repos=NULL, type="source")
library(parrot)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
# install locally (with edits)
install.packages("/Users/yoni/Repositories/parrot", repos=NULL, type="source")
# install locally (with edits)
install.packages("/Users/yoni/Repositories/parrot", repos=NULL, type="source")
# install locally (with edits)
detach("package:parrot", unload=TRUE)
install.packages("/Users/yoni/Repositories/parrot", repos=NULL, type="source")
# install locally (with edits)
detach("package:parrot", unload=TRUE)
install.packages("/Users/yoni/Repositories/parrot/R", repos=NULL, type="source")
install.packages("/Users/yoni/Repositories/parrot", repos=NULL, type="source")
install_github("wilryh/parrot", dependencies=TRUE)
# install locally (with edits) -- failing
detach("package:parrot", unload=TRUE)
install.packages("/Users/yoni/Repositories/parrot", repos=NULL, type="source", dependencies=TRUE)
install_github("wilryh/parrot", dependencies=TRUE)
library(parrot)
plot_keywords(
scores, x_dimension=1, y_dimension=2, q_cutoff=0.8, plot_density=F, color=TRUE
)
View(document_scores)
View(document_scores)
quit()
